{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator.DESKTOP-5L8D4BK\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 (1,)\n"
     ]
    }
   ],
   "source": [
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 200\n",
    "MAX_EP_STEPS = 200\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.002    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "TAU = 0.01      # soft replacement\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "RENDER = False\n",
    "ENV_NAME = 'Pendulum-v1'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "a_bound = env.action_space.high\n",
    "\n",
    "print(s_dim,a_dim,np.shape(a_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator.DESKTOP-5L8D4BK\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "C:\\Users\\Administrator.DESKTOP-5L8D4BK\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "###############################  LSTM-DQN  ####################################\n",
    "\n",
    "class LSTM_DQN(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        num_nodes = 64 \n",
    "        \n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n",
    "        \n",
    "        with tf.variable_scope(\"input\", initializer=tf.truncated_normal_initializer(-0.1, 0.1)) as input_layer:\n",
    "            self.ix, self.im, self.ib = self._generate_w_b(\n",
    "                x_weights_size=[s_dim, num_nodes],\n",
    "                m_weights_size=[s_dim, num_nodes],\n",
    "                biases_size=[1, num_nodes])\n",
    "        with tf.variable_scope(\"memory\", initializer=tf.truncated_normal_initializer(-0.1, 0.1)) as update_layer:\n",
    "            self.cx, self.cm, self.cb = self._generate_w_b(\n",
    "                x_weights_size=[s_dim, num_nodes],\n",
    "                m_weights_size=[s_dim, num_nodes],\n",
    "                biases_size=[1, num_nodes])\n",
    "        with tf.variable_scope(\"forget\", initializer=tf.truncated_normal_initializer(-0.1, 0.1)) as forget_layer:\n",
    "            self.fx, self.fm, self.fb = self._generate_w_b(\n",
    "                x_weights_size=[s_dim, num_nodes],\n",
    "                m_weights_size=[s_dim, num_nodes],\n",
    "                biases_size=[1, num_nodes])\n",
    "        with tf.variable_scope(\"output\", initializer=tf.truncated_normal_initializer(-0.1, 0.1)) as output_layer:\n",
    "            self.ox, self.om, self.ob = self._generate_w_b(\n",
    "                x_weights_size=[s_dim, num_nodes],\n",
    "                m_weights_size=[s_dim, num_nodes],\n",
    "                biases_size=[1, num_nodes])\n",
    "\n",
    "        self.saved_output = tf.Variable(tf.zeros([1, 3]), trainable=False)\n",
    "        self.saved_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "        \n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.a = self._run(self.S, scope='eval', trainable=True)\n",
    "            a_ = self._run(self.S_, scope='eval_', trainable=False)\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            self.q = self._build_c(self.S, self.a, scope='eval', trainable=True)\n",
    "            q_ = self._build_c(self.S_, a_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.a_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        self.soft_replace = [tf.assign(t, (1 - TAU) * t + TAU * e)\n",
    "                             for t, e in zip(self.ct_params, self.ce_params)]\n",
    "        \n",
    "        q_target = self.R + GAMMA * q_\n",
    "        \n",
    "        \n",
    "        td_error = tf.losses.mean_squared_error(labels=q_target, predictions=self.q)\n",
    "        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, var_list=self.ce_params)\n",
    "        \n",
    "        a_loss = -tf.reduce_mean(self.q)    # maximize the q\n",
    "        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.a_params)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    def _generate_w_b(self, x_weights_size, m_weights_size, biases_size):\n",
    "        x_w = tf.get_variable(\"x_weights\", x_weights_size, trainable=True)\n",
    "        m_w = tf.get_variable(\"m_weigths\", m_weights_size, trainable=True)\n",
    "        b = tf.get_variable(\"biases\", a_dim, initializer=tf.constant_initializer(0.0), trainable=True)\n",
    "        return x_w, m_w, b\n",
    "\n",
    "    def _run(self, input, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            state = self.saved_state\n",
    "            output = self.saved_output\n",
    "            forget_gate = tf.sigmoid(tf.matmul(input, self.fx) + tf.matmul(output, self.fm) + self.fb)\n",
    "            input_gate = tf.sigmoid(tf.matmul(input, self.ix) + tf.matmul(output, self.im) + self.ib)\n",
    "            update = tf.matmul(input, self.cx) + tf.matmul(output, self.cm) + self.cb\n",
    "            state = state * forget_gate + tf.tanh(update) * input_gate\n",
    "            output_gate = tf.sigmoid(tf.matmul(input, self.ox) + tf.matmul(output, self.om) + self.ob)\n",
    "            res = output_gate * tf.tanh(state)\n",
    "            self.saved_state = state\n",
    "            self.saved_output = output\n",
    "            \n",
    "            net = tf.layers.dense(res, 32, activation=tf.nn.relu, name='l1', trainable=trainable)\n",
    "            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name='a', trainable=trainable)\n",
    "\n",
    "            return tf.multiply(a, self.a_bound, name='scaled_a')    \n",
    "\n",
    "    \n",
    "    \n",
    "    def choose_action(self, s):\n",
    "        return self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]\n",
    "\n",
    "    def learn(self):\n",
    "        self.sess.run(self.soft_replace)\n",
    "\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n",
    "        br = bt[:, -self.s_dim - 1: -self.s_dim]\n",
    "        bs_ = bt[:, -self.s_dim:]\n",
    "        \n",
    "        self.sess.run(self.atrain, {self.S: bs})\n",
    "        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n",
    "#         print(self.sess.run(self.q,{self.S: bs, self.a: ba}))\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "\n",
    "    def _build_c(self, s, a, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 30\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n",
    "            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "            return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n",
    "        \n",
    "ldqn = LSTM_DQN(a_dim, s_dim, a_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Reward: -1152 Explore: 2.00\n",
      "Episode: 1  Reward: -1230 Explore: 2.00\n",
      "Episode: 2  Reward: -1012 Explore: 2.00\n",
      "Episode: 3  Reward: -1638 Explore: 2.00\n",
      "Episode: 4  Reward: -1068 Explore: 2.00\n",
      "Episode: 5  Reward: -1544 Explore: 2.00\n",
      "Episode: 6  Reward: -1240 Explore: 2.00\n",
      "Episode: 7  Reward: -1549 Explore: 2.00\n",
      "Episode: 8  Reward: -942 Explore: 2.00\n",
      "Episode: 9  Reward: -1163 Explore: 2.00\n",
      "Episode: 10  Reward: -985 Explore: 2.00\n",
      "Episode: 11  Reward: -1288 Explore: 2.00\n",
      "Episode: 12  Reward: -1677 Explore: 2.00\n",
      "Episode: 13  Reward: -1286 Explore: 2.00\n",
      "Episode: 14  Reward: -973 Explore: 2.00\n",
      "Episode: 15  Reward: -1762 Explore: 2.00\n",
      "Episode: 16  Reward: -1330 Explore: 2.00\n",
      "Episode: 17  Reward: -1117 Explore: 2.00\n",
      "Episode: 18  Reward: -969 Explore: 2.00\n",
      "Episode: 19  Reward: -1164 Explore: 2.00\n",
      "Episode: 20  Reward: -1703 Explore: 2.00\n",
      "Episode: 21  Reward: -1230 Explore: 2.00\n",
      "Episode: 22  Reward: -769 Explore: 2.00\n",
      "Episode: 23  Reward: -1376 Explore: 2.00\n",
      "Episode: 24  Reward: -944 Explore: 2.00\n",
      "Episode: 25  Reward: -1329 Explore: 2.00\n",
      "Episode: 26  Reward: -1171 Explore: 2.00\n",
      "Episode: 27  Reward: -1201 Explore: 2.00\n",
      "Episode: 28  Reward: -871 Explore: 2.00\n",
      "Episode: 29  Reward: -948 Explore: 2.00\n",
      "Episode: 30  Reward: -984 Explore: 2.00\n",
      "Episode: 31  Reward: -1348 Explore: 2.00\n",
      "Episode: 32  Reward: -891 Explore: 2.00\n",
      "Episode: 33  Reward: -901 Explore: 2.00\n",
      "Episode: 34  Reward: -875 Explore: 2.00\n",
      "Episode: 35  Reward: -1418 Explore: 2.00\n",
      "Episode: 36  Reward: -856 Explore: 2.00\n",
      "Episode: 37  Reward: -1076 Explore: 2.00\n",
      "Episode: 38  Reward: -1177 Explore: 2.00\n",
      "Episode: 39  Reward: -959 Explore: 2.00\n",
      "Episode: 40  Reward: -856 Explore: 2.00\n",
      "Episode: 41  Reward: -1317 Explore: 2.00\n",
      "Episode: 42  Reward: -1225 Explore: 2.00\n",
      "Episode: 43  Reward: -1178 Explore: 2.00\n",
      "Episode: 44  Reward: -1089 Explore: 2.00\n",
      "Episode: 45  Reward: -1626 Explore: 2.00\n",
      "Episode: 46  Reward: -856 Explore: 2.00\n",
      "Episode: 47  Reward: -1553 Explore: 2.00\n",
      "Episode: 48  Reward: -1017 Explore: 2.00\n",
      "Episode: 49  Reward: -1481 Explore: 2.00\n",
      "Episode: 50  Reward: -1203 Explore: 1.81\n",
      "Episode: 51  Reward: -1341 Explore: 1.64\n",
      "Episode: 52  Reward: -1678 Explore: 1.48\n",
      "Episode: 53  Reward: -1744 Explore: 1.34\n",
      "Episode: 54  Reward: -1600 Explore: 1.21\n",
      "Episode: 55  Reward: -1255 Explore: 1.10\n",
      "Episode: 56  Reward: -1094 Explore: 0.99\n",
      "Episode: 57  Reward: -1044 Explore: 0.90\n",
      "Episode: 58  Reward: -1221 Explore: 0.81\n",
      "Episode: 59  Reward: -1402 Explore: 0.74\n",
      "Episode: 60  Reward: -1357 Explore: 0.67\n",
      "Episode: 61  Reward: -128 Explore: 0.60\n",
      "Episode: 62  Reward: -491 Explore: 0.54\n",
      "Episode: 63  Reward: -130 Explore: 0.49\n",
      "Episode: 64  Reward: -258 Explore: 0.45\n",
      "Episode: 65  Reward: -258 Explore: 0.40\n",
      "Episode: 66  Reward: -257 Explore: 0.37\n",
      "Episode: 67  Reward: -253 Explore: 0.33\n",
      "Episode: 68  Reward: -245 Explore: 0.30\n",
      "Episode: 69  Reward: -249 Explore: 0.27\n",
      "Episode: 70  Reward: -127 Explore: 0.24\n",
      "Episode: 71  Reward: -262 Explore: 0.22\n",
      "Episode: 72  Reward: -261 Explore: 0.20\n",
      "Episode: 73  Reward: -127 Explore: 0.18\n",
      "Episode: 74  Reward: -131 Explore: 0.16\n",
      "Episode: 75  Reward: -270 Explore: 0.15\n",
      "Episode: 76  Reward: -371 Explore: 0.13\n",
      "Episode: 77  Reward: -127 Explore: 0.12\n",
      "Episode: 78  Reward: -369 Explore: 0.11\n",
      "Episode: 79  Reward: -241 Explore: 0.10\n",
      "Episode: 80  Reward: -369 Explore: 0.09\n",
      "Episode: 81  Reward: 0 Explore: 0.08\n",
      "Episode: 82  Reward: -264 Explore: 0.07\n",
      "Episode: 83  Reward: -356 Explore: 0.07\n",
      "Episode: 84  Reward: -125 Explore: 0.06\n",
      "Episode: 85  Reward: -124 Explore: 0.05\n",
      "Episode: 86  Reward: -132 Explore: 0.05\n",
      "Episode: 87  Reward: -125 Explore: 0.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a04fe60d39ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mldqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpointer\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mMEMORY_CAPACITY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mvar\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m.9995\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mldqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mep_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-bc86a4097a03>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mbs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbs_\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;31m#         print(self.sess.run(self.q,{self.S: bs, self.a: ba}))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1369\u001b[0m                            run_metadata)\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1373\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1360\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1449\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1450\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1451\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "var = 2\n",
    "for episode in range(MAX_EPISODES):\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        a = ldqn.choose_action(s)\n",
    "        a = np.clip(np.random.normal(a, var), -2, 2)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        ldqn.store_transition(s, a, r/10, s_)\n",
    "        if ldqn.pointer > MEMORY_CAPACITY:\n",
    "            var *= .9995\n",
    "            ldqn.learn()\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "        \n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            print('Episode:', episode, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % var)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
